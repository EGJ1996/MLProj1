{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Project 1 : Higgs Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from myhelpers import *\n",
    "\n",
    "data_path = \"datasets/train.csv\"\n",
    "yb, input_data, ids = load_csv_data(data_path, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data \n",
    "We set to 0 the invalid values and standardize the data by columns in order to deal with dimensionless variables that span more or less the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansubx = input_data\n",
    "cleansubx[np.where(cleansubx == -999)] = 0\n",
    "means_by_columns = np.mean(cleansubx, axis=0)\n",
    "\n",
    "for i in range(30):\n",
    "    input_data[:,np.where(input_data[:,i]==-999)] = means_by_columns[i]\n",
    "    \n",
    "input_data = standardize(input_data)\n",
    "rows, cols = input_data.shape[0], input_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficients of the features, to understand if they are linearly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_matrix = np.corrcoef(input_data.T)\n",
    "\n",
    "plt.figure(1, figsize=(12,12))\n",
    "plt.matshow(np.abs(cor_matrix),1)\n",
    "plt.xticks(range(cols), range(cols))\n",
    "plt.yticks(range(cols), range(cols))\n",
    "plt.colorbar()\n",
    "plt.savefig('correlation_matrix'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to leave out columns 5, 9, 12, 21, 22, 23, 29 due to the high correlation with the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.delete(input_data,[5,9,12,21,22,23,29],1)\n",
    "\n",
    "#Compute again the correlation and check\n",
    "cor_matrix = np.corrcoef(data.T)\n",
    "plt.figure(2, figsize=(12,12))\n",
    "plt.matshow(np.abs(cor_matrix),2)\n",
    "plt.xticks(range(23), range(23))\n",
    "plt.yticks(range(23), range(23))\n",
    "plt.colorbar()\n",
    "plt.savefig('correlation_new'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a linear model\n",
    "First of all we fit our linear model on the subset of data, and then use the whole train dataset to test it, at least at the beginning. In the end we will use the whole dataset to train the model. We try with 5 polynomial degrees per feature, otherwise the number of features becomes too large.\n",
    "Compare the mse and the computational time of the 3 methods to get the least squares solution: the first one is the correct one, but maybe the others are faster.\n",
    "\n",
    "\n",
    "1.Linear regression with mse and normal equations\n",
    "\n",
    "\n",
    "2.Linear regression with mse and gradient descent\n",
    "\n",
    "\n",
    "3.Linear regression with mse and stochastic gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that builds a polynomial basis of the chosen degree for a column \n",
    "def build_poly_col(x, degree):\n",
    "    \"\"\"polynomial basis functions for input column x, for j=1 up to j=degree.\"\"\"\n",
    "    \n",
    "    y = x\n",
    "        \n",
    "    for n in range(2,degree+1):\n",
    "        x = np.c_[x, np.power(y,n)]\n",
    "            \n",
    "    return x\n",
    "\n",
    "# Function that builds a polynomial basis for the data (each column has the same degree)\n",
    "def build_poly(data, degree):\n",
    "    \"\"\"polynomial basis for input data up to the chosen degree in each column\"\"\"\n",
    "    \n",
    "    X = np.c_[np.ones(data.shape[0])]\n",
    "    for j in range(data.shape[1]):\n",
    "        x_col = build_poly_col(data[:,j], degree)\n",
    "        X = np.c_[X, x_col]\n",
    "        \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to build the indices and split the data to perform cross validation\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    \n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def split_dataset(y, x, k, k_indices):\n",
    "    \"\"\"Returns the matrices and vectors Test and Train used in a k-fold cross validation\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train.\n",
    "    X_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    X_train = np.delete(x,k_indices[k],0)\n",
    "    y_train = np.delete(y,k_indices[k])\n",
    "    \n",
    "    return X_test, y_test, X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ls_polynomial_basis(yb, data, kfolds, degrees):\n",
    "    \"\"\"Builds a polynomial basis according to the chosen degree and solves the least squares problem\n",
    "        with the normal equations, performing a k-fold cross validation to compute the rmse and the weights\"\"\"\n",
    "    \n",
    "    rmse_train=[]\n",
    "    rmse_test=[]\n",
    "    seed = 2\n",
    "\n",
    "    # Build indices to split data\n",
    "    k_indices = build_k_indices(yb, kfolds, seed)\n",
    "\n",
    "    # Define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    w_ls = []\n",
    "\n",
    "    # Choose the degrees and loop over them\n",
    "    for degree in degrees:\n",
    "        rmse_tr_k=[]\n",
    "        rmse_te_k=[]\n",
    "        w_ls_deg = []\n",
    "    \n",
    "        # Build X matrix\n",
    "        data1 = build_poly(data, degree)\n",
    "    \n",
    "        # Loop over the folds\n",
    "        for k in range(kfolds):\n",
    "        \n",
    "            data_test_ls, y_test_ls, data_train_ls, y_train_ls = split_dataset(yb, data1, k, k_indices)   \n",
    "        \n",
    "            # Train and store rmse\n",
    "            w_ls_k, mse_ls_k = least_squares(y_train_ls, data_train_ls)\n",
    "            rmse_test = np.sqrt(2*compute_mse(y_test_ls, data_test_ls, w_ls_k))\n",
    "            \n",
    "            rmse_tr_k.append(np.sqrt(2*mse_ls_k))\n",
    "            rmse_te_k.append(rmse_test)\n",
    "            w_ls_deg.append(w_ls_k)\n",
    "            \n",
    "        \n",
    "        # Append rmse (and std over the folds) in the vectors\n",
    "        rmse_tr.append(np.mean(rmse_tr_k))\n",
    "        rmse_te.append(np.mean(rmse_te_k))\n",
    "        std_tr.append(np.std(rmse_tr_k))\n",
    "        std_te.append(np.std(rmse_te_k))\n",
    "        w_ls.append(np.mean(w_ls_deg, axis=0))\n",
    "        \n",
    "\n",
    "    return rmse_tr, rmse_te, std_tr, std_te, w_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try this first simple model both on the matrix with all the features and on the reduced matrix, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1,5)\n",
    "\n",
    "rmse_train_tot, rmse_test_tot, std_train_tot, std_test_tot, w_tot = ls_polynomial_basis(yb, input_data, 5, degrees)\n",
    "rmse_train_red, rmse_test_red, std_train_red, std_test_red, w_red = ls_polynomial_basis(yb, data, 5, degrees)\n",
    "\n",
    "# Plot RMSE to compare the results\n",
    "fig3, axs= plt.subplots(1,2, figsize=(14,6))\n",
    "axs[0].plot(degrees, rmse_train_tot, marker=\".\", c='b', label='Train error')\n",
    "axs[0].plot(degrees, rmse_test_tot, marker=\".\", c='r', label='Test error')\n",
    "#plt.errorbar(degrees, rmse_tr, std_tr, marker=\".\", color='b', label='Train error')\n",
    "#plt.errorbar(degrees, rmse_te, std_te, marker=\".\", color='r', label='Test error')\n",
    "axs[0].set_xlabel(\"Degrees\")\n",
    "axs[0].set_ylabel(\"RMSE\")\n",
    "axs[0].set_title(\"RMSE Least Squares polynomial basis TOTAL\")\n",
    "axs[0].legend(loc=2)\n",
    "axs[0].set_xticks(degrees)\n",
    "\n",
    "axs[1].plot(degrees, rmse_train_red, marker=\".\", c='b', label='Train error')\n",
    "axs[1].plot(degrees, rmse_test_red, marker=\".\", c='r', label='Test error')\n",
    "axs[1].set_xlabel(\"Degrees\")\n",
    "axs[1].set_ylabel(\"RMSE\")\n",
    "axs[1].set_title(\"RMSE Least Squares polynomial basis REDUCED\")\n",
    "axs[1].legend(loc=2)\n",
    "axs[1].set_xticks(degrees)\n",
    "plt.savefig(\"rmse_ls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training RMSE LS with NORMAL EQUATIONS: {err1}'.format(err1=rmse_train_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we find a significant overfitting even at a low degree (3,4) and to try to reduce it we use ridge regression. Before doing that, we try to minimize the mse loss function with GD and SGD: we should get an approximation of the minimum (compare the values with the exact value give by LS!) but maybe save computational time. However we should put a lot of effort in tuning the parameters for the iterative method (max_iter, gamma) and maybe it's not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "max_iters = 2000\n",
    "\n",
    "# build the matrix and call the methods, after setting the parameters to ensure convergence\n",
    "data2 = build_poly(data,2)\n",
    "w_initial_red = np.zeros(data2.shape[1])\n",
    "\n",
    "w_red_GD, mse_red_GD = least_squares_GD(yb, data2, w_initial_red, max_iters, 0.001, tol=1e-4)\n",
    "w_red_SGD, mse_red_SGD = least_squares_SGD(yb, data2, w_initial_red, max_iters, 0.001, tol=1e-5)\n",
    "\n",
    "rmse_train_red_GD = np.sqrt(2*mse_red_GD)\n",
    "rmse_train_red_SGD = np.sqrt(2*mse_red_SGD)\n",
    "\n",
    "print('Training RMSE LS with NORMAL EQUATIONS: {err1}'.format(err1=rmse_train_red[1]))\n",
    "print('Training RMSE LS with GRADIENT DESCENT: {err2}'.format(err2=rmse_train_red_GD))\n",
    "print('Training RMSE LS with STOCHASTIC GRADIENT DESCENT: {err3}'.format(err3=rmse_train_red_SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distance between the weigths normal equations/GD: {dist1}'.format(dist1=np.linalg.norm(w_red[1] - w_red_GD)))\n",
    "print('Distance between the weigths normal equations/SGD: {dist2}'.format(dist2=np.linalg.norm(w_red[1] - w_red_SGD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see any useful improvement by using the approximate methods to solve the minimization problem: the error on the training set is higher (correct) even if the one with GD is comparable, the computational time is even higher and moreover we need to run different attempts in order to decide the suitable parameters max_iters and gamma to obtain convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression.\n",
    "Since we had the problem of overfitting, we try to introduce a regularization technique, based again on the mse loss function.\n",
    "We try with different lambdas and do a 5 folds cross validation with a fixed degree (=2), then we try again changing the degree because this is exactly the purpose of the regularization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(ypred, yknown):\n",
    "    \"\"\"Computes the accuracy of the prediction as n_exact_predictions/total\"\"\"\n",
    "    \n",
    "    nexact = np.sum(ypred == yknown)\n",
    "    return nexact/len(yknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_lambda(yb, data, kfolds, degree, lambdas):\n",
    "    \"\"\"Builds a polynomial basis according to the chosen degree and solves the ridge regression\n",
    "        with the normal equations for all the lambdas in lambdas, performing a k-fold cross validation \n",
    "        to compute the rmse and the weights\"\"\"\n",
    "    \n",
    "    rmse_train=[]\n",
    "    rmse_test=[]\n",
    "    seed = 3\n",
    "\n",
    "    # Build indices to split data\n",
    "    k_indices = build_k_indices(yb, kfolds, seed)\n",
    "\n",
    "    # Define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    w_ridge = []\n",
    "    accuracy = []\n",
    "\n",
    "    # Loop over the lambdas\n",
    "    for lam in lambdas:\n",
    "        rmse_tr_k=[]\n",
    "        rmse_te_k=[]\n",
    "        w_ridge_lam = []\n",
    "        accuracy_lam = []\n",
    "        \n",
    "        # Build X matrix\n",
    "        data1 = build_poly(data, degree)\n",
    "    \n",
    "        # Loop over the folds\n",
    "        for k in range(kfolds):\n",
    "        \n",
    "            data_test, y_test, data_train, y_train = split_dataset(yb, data1, k, k_indices)   \n",
    "        \n",
    "            # Train the model\n",
    "            w_k, mse_k = ridge_regression(y_train, data_train, lam)\n",
    "            rmse_test = np.sqrt(2*compute_mse(y_test, data_test, w_k))\n",
    "            \n",
    "            # Do the prediction on the test set and compute accuracy\n",
    "            ypred = predict_labels(w_k, data_test)\n",
    "            acc_k = compute_accuracy(ypred, y_test)\n",
    "            \n",
    "            # Store rmse, weights and accuracy\n",
    "            rmse_tr_k.append(np.sqrt(2*mse_k))\n",
    "            rmse_te_k.append(rmse_test)\n",
    "            w_ridge_lam.append(w_k)\n",
    "            accuracy_lam.append(acc_k)\n",
    "        \n",
    "        # Append rmse (and std over the folds) in the lists\n",
    "        rmse_tr.append(np.mean(rmse_tr_k))\n",
    "        rmse_te.append(np.mean(rmse_te_k))\n",
    "        std_tr.append(np.std(rmse_tr_k))\n",
    "        std_te.append(np.std(rmse_te_k))\n",
    "        w_ridge.append(np.mean(w_ridge_lam, axis=0))\n",
    "        accuracy.append(np.mean(accuracy_lam))\n",
    "        \n",
    "\n",
    "    return rmse_tr, rmse_te, std_tr, std_te, w_ridge, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_lambda_vis(lambdas, rmse_tr, rmse_te, std_train, std_test, acc, deg):\n",
    "    \"\"\"visualization of the curves of rmse_tr and rmse_te, and of the accuracy\"\"\"\n",
    "    \n",
    "    plt.figure(4, figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.semilogx(lambdas, rmse_tr, marker=\".\", color='b', label='Train error')\n",
    "    plt.semilogx(lambdas, rmse_te, marker=\".\", color='r', label='Test error')\n",
    "    #plt.errorbar(np.log10(lambdas), rmse_tr, std_train, label='Train error')\n",
    "    #plt.errorbar(np.log10(lambdas), rmse_te, std_test, label='Test error')\n",
    "    #plt.plot(lambdas, rmse_tr, marker=\".\", color='b', label='Train error')\n",
    "    #plt.plot(lambdas, rmse_te, marker=\".\", color='r', label='Test error')\n",
    "    plt.xlabel(\"Lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE ridge regression with degree {ddd}\".format(ddd=deg))\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.semilogx(lambdas, acc,  marker=\".\", color='b', label='Accuracy')\n",
    "    plt.xlabel(\"Lambda\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"ACCURACY ridge regression with degree {ddd}\".format(ddd=deg))\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 4\n",
    "kfolds = 5\n",
    "lambdas = np.logspace(-6,1,30)\n",
    "\n",
    "rmse_ridge_train, rmse_ridge_test, std_train, std_test, w_ridge, accuracy = ridge_lambda(yb, data, kfolds, degree, lambdas)\n",
    "ridge_lambda_vis(lambdas, rmse_ridge_train, rmse_ridge_test, std_train, std_test, accuracy, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search.\n",
    "To find the best couple (lambda, degree): I run it on a subset of the data (to save computational time) and then on the whole matrix, and check the difference in the rmse test.\n",
    "\n",
    "1.Define ranges for lambda and degree.\n",
    "\n",
    "2.Loop over the degrees.\n",
    "\n",
    "3.Call the function ridge_lambda(). \n",
    "\n",
    "4.Save rmse test in a matrix: rows=degree, cols=lambdas.\n",
    "\n",
    "5.Get the minimum and the associated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the engineered data to save computational time\n",
    "yb_grid = yb[::25]\n",
    "data_grid = data[::25]\n",
    "ids_grid= ids[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for the parameters \n",
    "degree_grid = range(1,15)\n",
    "lambdas_grid = np.linspace(0,1,50)\n",
    "kfolds = 5\n",
    "\n",
    "losses = np.zeros((len(degree_grid),len(lambdas_grid)))\n",
    "accuracies = np.zeros((len(degree_grid),len(lambdas_grid)))\n",
    "\n",
    "# Perform a grid search\n",
    "for i, deg in enumerate(degree_grid):\n",
    "        \n",
    "        rmse_ridge_train, rmse_ridge_test, std_train, std_test, w_ridge, acc = ridge_lambda(yb_grid, data_grid, kfolds, deg, lambdas_grid)\n",
    "        losses[i,:] = rmse_ridge_test\n",
    "        accuracies[i,:] = acc\n",
    "        \n",
    "min_rmse_test, deg_opt_err, lam_opt_err = get_best_rmse(lambdas_grid, degree_grid, losses)  \n",
    "max_accuracy, deg_opt_acc, lam_opt_acc = get_best_accuracy(lambdas_grid, degree_grid, accuracies)\n",
    "\n",
    "print('Minimum Test RMSE (on the subset): {rmse}'.format(rmse=min_rmse_test))\n",
    "print('Optimal degree: {d}'.format(d=deg_opt_err))\n",
    "print('Optimal lambda: {l}'.format(l=lam_opt_err))\n",
    "\n",
    "print('Maximum accuracy (on the subset): {acc}'.format(acc=max_accuracy))\n",
    "print('Optimal degree: {d}'.format(d=deg_opt_acc))\n",
    "print('Optimal lambda: {l}'.format(l=lam_opt_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_parameters(lam, deg, losses):\n",
    "    \"\"\"Get the best (lam, deg) from the result of grid search.\"\"\"\n",
    " \n",
    "    max_row, max_col = np.unravel_index(np.argmax(losses), losses.shape)\n",
    "    return losses[max_row, max_col], deg[max_row], lam[max_col]\n",
    "\n",
    "max_rmse, deg_wor, lam_wor = get_worst_parameters(lambdas_grid, degree_grid, losses)\n",
    "print('Maximum Test RMSE (on the subset): {rmse}'.format(rmse=max_rmse))\n",
    "print('Worst degree: {d}'.format(d=deg_wor))\n",
    "print('Worst lambda: {l}'.format(l=lam_wor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try again with another subset of data and see the differences in the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load another subset of the engineered data to save computational time\n",
    "yb_grid1 = yb[::10]\n",
    "data_grid1 = data[::10]\n",
    "ids_grid1 = ids[::10]\n",
    "\n",
    "# Define the range for the parameters \n",
    "degree_grid = range(1,10)\n",
    "lambdas_grid = np.linspace(-1,2,40)\n",
    "kfolds = 5\n",
    "\n",
    "losses = np.zeros((len(degree_grid),len(lambdas_grid)))\n",
    "accuracies = np.zeros((len(degree_grid),len(lambdas_grid)))\n",
    "\n",
    "# Perform a grid search\n",
    "for i, deg in enumerate(degree_grid):\n",
    "        \n",
    "        rmse_ridge_train, rmse_ridge_test, std_train, std_test, w_ridge, acc = ridge_lambda(yb_grid1, data_grid1, kfolds, deg, lambdas_grid)\n",
    "        losses[i,:] = rmse_ridge_test\n",
    "        accuracies[i,:] = acc\n",
    "        \n",
    "min_rmse_test1, deg_opt1, lam_opt1 = get_best_rmse(lambdas_grid, degree_grid, losses)   \n",
    "max_accuracy1, deg_opt_acc1, lam_opt_acc1 = get_best_accuracy(lambdas_grid, degree_grid, accuracies)\n",
    "\n",
    "print('Minimum Test RMSE (on the second subset): {rmse}'.format(rmse=min_rmse_test1))\n",
    "print('Optimal degree: {d}'.format(d=deg_opt1))\n",
    "print('Optimal lambda: {l}'.format(l=lam_opt1))\n",
    "\n",
    "print('Maximum accuracy (on the second subset): {acc}'.format(acc=max_accuracy1))\n",
    "print('Optimal degree: {d}'.format(d=deg_opt_acc1))\n",
    "print('Optimal lambda: {l}'.format(l=lam_opt_acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... I tried different dimensions of the subset (to get the best parameters) and even the total dataset, since the computational time wasn't too high. In the end I decided to keep the optimal values obtained with the whole dataset and perform another cross validation (same seed and kfolds) in order to get the best weights and used them in the prediction. If I use different seeds and kfolds I get different values for the error at each try, so I decided to keep the same, in order to doublecheck the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 6\n",
    "\n",
    "rmse_train=[]\n",
    "rmse_test=[]\n",
    "seed = 2\n",
    "\n",
    "# Build indices to split data\n",
    "k_indices = build_k_indices(yb, kfolds, seed)\n",
    "\n",
    "# Define lists to store the loss of training data and test data\n",
    "rmse_opt_tr = []\n",
    "rmse_opt_te = []\n",
    "w_opt_ridge_list = []\n",
    "acc_opt = []\n",
    "\n",
    "# Build X matrix\n",
    "data1 = build_poly(data, deg_opt_acc1)\n",
    "\n",
    "# Loop over the folds\n",
    "for k in range(kfolds):\n",
    "    \n",
    "    data_test, y_test, data_train, y_train = split_dataset(yb, data1, k, k_indices)   \n",
    "        \n",
    "    # Train the model\n",
    "    w_k, mse_k = ridge_regression(y_train, data_train, lam_opt_acc1)\n",
    "    rmse_test = np.sqrt(2*compute_mse(y_test, data_test, w_k))\n",
    "    \n",
    "    # Do the prediction on the test set and compute accuracy\n",
    "    ypred = predict_labels(w_k, data_test)\n",
    "    acc_k = compute_accuracy(ypred, y_test)\n",
    "            \n",
    "    # Store rmse and weights\n",
    "    rmse_opt_tr.append(np.sqrt(2*mse_k))\n",
    "    rmse_opt_te.append(rmse_test)\n",
    "    w_opt_ridge_list.append(w_k)\n",
    "    acc_opt.append(acc_k)\n",
    "    \n",
    "# Get the best weights and the errors\n",
    "w_opt_ridge = np.mean(w_opt_ridge_list, axis=0)\n",
    "\n",
    "print('Test RMSE: {te}'.format(te=np.mean(rmse_opt_te)))\n",
    "print('Train RMSE: {tr}'.format(tr=np.mean(rmse_opt_tr)))\n",
    "print('Accuracy: {acc}'.format(acc=np.mean(acc_opt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE values with deg=2 and lambda=0.0011937766417144358, subset 1:25 :\n",
    "Train RMSE: 0.8014915854773715\n",
    "Test RMSE: 0.8305541106371641\n",
    "\n",
    "RMSE values with deg=3 and lambda=0.007038135554931562, subset 1:20 :\n",
    "Train RMSE: 0.793285618340946\n",
    "Test RMSE: 1.3284903246013344\n",
    "\n",
    "RMSE values with deg=2 and lambda=0.0002894266124716752, subset 1:15 :\n",
    "Train RMSE: 0.801442221422809\n",
    "Test RMSE: 0.8276588788503378\n",
    "\n",
    "RMSE values with deg=4 and lambda=0.08376776400682924, subset 1:10 :\n",
    "Train RMSE: 0.7945378819037896\n",
    "Test RMSE: 17.95365044601708\n",
    "\n",
    "RMSE values with deg=3 and lambda=0.11689518164985777, whole dataset:\n",
    "Train RMSE: 0.8013123911573933\n",
    "Test RMSE: 0.8075341744448667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a submission!!! %---the last one was with the accuracy---%\n",
    "data_path_sub = \"datasets/test.csv\"\n",
    "yb_sub, input_data_sub, ids_sub = load_csv_data(data_path_sub, sub_sample=False)\n",
    "\n",
    "#preparing the matrix\n",
    "cleansubx = input_data_sub\n",
    "cleansubx[np.where(cleansubx == -999)] = 0\n",
    "means_by_columns = np.mean(cleansubx, axis=0)\n",
    "\n",
    "for i in range(30):\n",
    "    input_data_sub[:,np.where(input_data_sub[:,i]==-999)] = means_by_columns[i]\n",
    "    \n",
    "input_data_sub = standardize(input_data_sub)\n",
    "data_sub = np.delete(input_data_sub,[5,9,12,21,22,23,29],1)\n",
    "\n",
    "data_sub1 = build_poly(data_sub, deg_opt_acc1)\n",
    "print(data_sub1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_opt_ridge.shape)\n",
    "ypred = predict_labels(w_opt_ridge, data_sub1)\n",
    "create_csv_submission(ids_sub,ypred,'ridgeaccuracy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "We wrote the 4 functions (logistic, reg_logistic, logistic_newton, reg_logistic_newton) and they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=50, loss=305.06557802074195, norm_grad = 108.54222128139581\n",
      "Current iteration=100, loss=285.73219496783855, norm_grad = 65.74816385627877\n",
      "Current iteration=150, loss=276.65176147364156, norm_grad = 39.820169947583814\n",
      "Current iteration=200, loss=272.58961017651774, norm_grad = 24.110054260931946\n",
      "Current iteration=250, loss=270.87865632332864, norm_grad = 14.594313037305483\n",
      "Current iteration=300, loss=270.19176676619696, norm_grad = 8.832611014807739\n",
      "Current iteration=350, loss=269.92509143516554, norm_grad = 5.344912576274182\n",
      "Current iteration=400, loss=269.8238199088845, norm_grad = 3.234128382669131\n",
      "Current iteration=450, loss=269.78589934555276, norm_grad = 1.9568248429082793\n",
      "Current iteration=500, loss=269.7718246522505, norm_grad = 1.183949116667007\n",
      "Current iteration=550, loss=269.76662896098856, norm_grad = 0.7163178298749061\n",
      "Current iteration=600, loss=269.7647173367197, norm_grad = 0.43338450498830977\n",
      "Current iteration=650, loss=269.76401542763676, norm_grad = 0.26220314240673726\n",
      "Current iteration=700, loss=269.7637580185659, norm_grad = 0.15863555024930798\n",
      "Current iteration=750, loss=269.7636636902548, norm_grad = 0.09597585802710489\n",
      "Current iteration=800, loss=269.7636291390294, norm_grad = 0.05806611836895123\n",
      "Current iteration=850, loss=269.76361648684076, norm_grad = 0.035130405886501376\n",
      "Current iteration=900, loss=269.7636118545511, norm_grad = 0.021254127731347716\n",
      "Current iteration=950, loss=269.76361015872203, norm_grad = 0.012858883195159373\n",
      "Current iteration=1000, loss=269.7636095379359, norm_grad = 0.007779704899594512\n",
      "Current iteration=1050, loss=269.76360931069536, norm_grad = 0.004706769598169765\n",
      "Current iteration=1100, loss=269.76360922751496, norm_grad = 0.0028476245051803728\n",
      "Current iteration=1150, loss=269.7636091970677, norm_grad = 0.0017228302276355767\n",
      "Current iteration=1200, loss=269.76360918592286, norm_grad = 0.0010423227861314573\n",
      "Current iteration=1250, loss=269.76360918184355, norm_grad = 0.0006306116262938748\n",
      "Current iteration=1300, loss=269.7636091803502, norm_grad = 0.0003815238660364651\n",
      "Current iteration=1350, loss=269.7636091798037, norm_grad = 0.00023082425595518714\n",
      "Current iteration=1400, loss=269.7636091796037, norm_grad = 0.00013965007609460198\n",
      "Current iteration=1450, loss=269.76360917953036, norm_grad = 8.448914363492e-05\n",
      "Current iteration=1500, loss=269.76360917950365, norm_grad = 5.1116444621836835e-05\n",
      "Current iteration=1550, loss=269.76360917949387, norm_grad = 3.092575914839713e-05\n",
      "Current iteration=1600, loss=269.76360917949023, norm_grad = 1.871027188327231e-05\n",
      "Current iteration=1650, loss=269.7636091794889, norm_grad = 1.131982804417651e-05\n",
      "Current iteration=1700, loss=269.76360917948836, norm_grad = 6.848564667891794e-06\n",
      "Current iteration=1750, loss=269.7636091794883, norm_grad = 4.143423126473773e-06\n",
      "Current iteration=1800, loss=269.7636091794881, norm_grad = 2.5067960510761233e-06\n",
      "Current iteration=1850, loss=269.7636091794881, norm_grad = 1.5166268136723351e-06\n",
      "2.1723028556802935\n"
     ]
    }
   ],
   "source": [
    "from myhelpers import *\n",
    "\n",
    "dataprova = data[::10]\n",
    "ybprova = yb[::10]\n",
    "ybprova[np.where(ybprova==-1)] = 0\n",
    "ids = ids[::10]\n",
    "\n",
    "maxit = 10000\n",
    "lambda_= 1\n",
    "initial_w = np.zeros(data.shape[1])\n",
    "gamma = 0.01\n",
    "w, loss = reg_logistic_newton(ybprova, dataprova,lambda_, initial_w, maxit, gamma, 1e-6)\n",
    "print(np.linalg.norm(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n",
    "Given the test data set, we need to split it according to the jet number (KEEPING THE ORIGINAL IDS), and then apply the same feature engineering techniques that we have on the train dataset (standardize, remove columns and build polynomials); make predictions according to the model and then rebuild the ypred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(y1, id1, y2, id2, y3, id3, y4, id4, total_ids, name_file):\n",
    "    \"\"\"Reconstruct the prediction on the whole dataset starting from the partial ones obtained with the different models\n",
    "    the name_file should be in the string format 'name.csv' \"\"\"\n",
    "    \n",
    "    ypred = np.zeros(len(total_ids))\n",
    "    \n",
    "    ypred[id1] = y1\n",
    "    ypred[id2] = y2\n",
    "    ypred[id3] = y3\n",
    "    ypred[id4] = y4\n",
    "    \n",
    "    create_csv_submission(total_ids, ypred, name_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the function\n",
    "tot = np.array(range(12))\n",
    "id1 = np.array([2,7,5])\n",
    "id2 = np.array([1,11,10])\n",
    "id3 = np.array([3,9,0])\n",
    "id4 = np.array([4,6,8])\n",
    "\n",
    "y1 = np.array([1,1,1])\n",
    "y2 = np.array([2,2,2])\n",
    "y3 = np.array([3,3,3])\n",
    "y4 = np.array([4,4,4])\n",
    "\n",
    "create_submission(y1, id1, y2, id2, y3, id3, y4, id4, tot, 'prova.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
